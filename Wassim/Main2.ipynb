{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db65cf4",
   "metadata": {},
   "source": [
    "WD - Presentation - 1st Topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da412c8e",
   "metadata": {},
   "source": [
    "DATA CLEANUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies and setup pathlib to load csv files\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as st\n",
    "import nasdaqdatalink as nasdaq\n",
    "import os\n",
    "\n",
    "api_key = os.path.join('Resources', 'nasdaq_api_key.txt')\n",
    "nasdaq.read_key(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e6a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://www.zillow.com/research/data/\n",
    "\n",
    "zillow_indicators = nasdaq.get_table(\"ZILLOW/INDICATORS\")\n",
    "\n",
    "indicator_inventory_sales_df = zillow_indicators.loc[zillow_indicators[\"category\"] == \"Inventory and sales\"]\n",
    "\n",
    "#indicator_inventory_sales_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aca0cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_regions = nasdaq.get_table(\"ZILLOW/REGIONS\", paginate = True)\n",
    "\n",
    "#zillow_regions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dd341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "zillow_iraw = nasdaq.get_table(\"ZILLOW/DATA\", indicator_id = 'IRAW', paginate = True)\n",
    "\n",
    "#zillow_iraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e982d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inner merge both datasets into one;\n",
    "# Only keep rows in the left DataFrame (zillow_iram) where value of 'region_id' exists on both DataFrames\n",
    "merged_df = pd.merge(zillow_iraw, zillow_regions, how = \"left\", on = [\"region_id\"])\n",
    "\n",
    "#merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15641d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[['City', 'State']] = merged_df['region'].str.split(', ', expand = True)\n",
    "\n",
    "#merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bfc966",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_unique_states(dataframe):\n",
    "    unique_states = dataframe[\"State\"].unique()\n",
    "    count_states = dataframe[\"State\"].nunique()\n",
    "\n",
    "    print(f\"There are {count_states} States found in the Merged DataFrame:\")\n",
    "\n",
    "    for state in unique_states :\n",
    "        print(state)\n",
    "\n",
    "\n",
    "print_unique_states(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bcf192",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in merged_df.iterrows() :\n",
    "    state_element = row[\"State\"]\n",
    "    \n",
    "    if state_element is not None and ';' in state_element :\n",
    "        new_state = state_element.split(';', 1)[0]\n",
    "        \n",
    "        merged_df.at[index, \"State\"] = new_state\n",
    "        \n",
    "\n",
    "merged_df = merged_df.dropna(subset = [\"State\"])\n",
    "\n",
    "print(f\"There are {len(merged_df)} rows in the DataFrame.\\n\")\n",
    "\n",
    "print_unique_states(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nan_values = merged_df.isnull().sum()\n",
    "\n",
    "print(columns_with_nan_values)\n",
    "\n",
    "print()\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dc26a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('./Resources/iraw_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554f302a",
   "metadata": {},
   "source": [
    "DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea5afa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv dataset file from 'Resources' folder\n",
    "load_iraw_data = Path(\"Resources/iraw_data.csv\")\n",
    "\n",
    "# Read iram_data csv file and store dataset into new Pandas DataFrame\n",
    "iraw_df = pd.read_csv(load_iraw_data, index_col = False)\n",
    "\n",
    "# Only Region Type listed in the Dataset is 'Metro'\n",
    "# Therefore, the scope of my dataset covers Active Sales Properties for Metropolitan USA\n",
    "iraw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21d89de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the United States Census Bureau, the USA can be broken down into 4 Regions.\n",
    "# Source: https://www2.census.gov/geo/pdfs/maps-data/maps/reference/us_regdiv.pdf\n",
    "\n",
    "# Northeast Region tuple = 9 States\n",
    "region_northeast = ('CT', 'MA', 'ME', 'NH', 'NJ', 'NY', 'PA', 'RI', 'VT')\n",
    "\n",
    "# Midwest Region tuple = 12 States\n",
    "region_midwest = ('IA', 'IL', 'IN', 'KS', 'MI', 'MN', 'MO', 'ND', 'NE', 'OH', 'SD', 'WI')\n",
    "\n",
    "# South Region tuple = 17 States (including District of Columbia i.e. DC)\n",
    "region_south = ('AL', 'AR', 'DC', 'DE', 'FL', 'GA', 'KY', 'LA', 'MD', 'MS', 'NC', 'OK', 'SC', 'TN', 'TX', 'VA', 'WV')\n",
    "\n",
    "# West Region tuple = 13 States\n",
    "region_west = ('AK', 'AZ', 'CA', 'CO', 'HI', 'ID', 'MT', 'NM', 'NV', 'OR', 'UT', 'WA', 'WY')\n",
    "\n",
    "# In the 'Census Region' column, add 'Northeast' for all rows if their 'State' value belongs in region_northeast\n",
    "iraw_df.loc[iraw_df['State'].isin(region_northeast), 'Census Region'] = \"Northeast\"\n",
    "\n",
    "# In the 'Census Region' column, add 'Midwest' for all rows if their 'State' value belongs in region_midwest\n",
    "iraw_df.loc[iraw_df['State'].isin(region_midwest), 'Census Region'] = \"Midwest\"\n",
    "\n",
    "# In the 'Census Region' column, add 'South' for all rows if their 'State' value belongs in region_south\n",
    "iraw_df.loc[iraw_df['State'].isin(region_south), 'Census Region'] = \"South\"\n",
    "\n",
    "# In the 'Census Region' column, add 'South' for all rows if their 'State' value belongs in region_west\n",
    "iraw_df.loc[iraw_df['State'].isin(region_west), 'Census Region'] = \"West\"\n",
    "\n",
    "iraw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8b605e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to obtain the season for the given date\n",
    "def get_season (date):\n",
    "    \n",
    "    # Get the month and day of current date\n",
    "    month = date.month\n",
    "    day = date.day\n",
    "   \n",
    "    # If Month is December or January or February, return Winter\n",
    "    if (month == 12) or (month <= 2) :\n",
    "        return 'Winter'\n",
    "    \n",
    "    # Else If Month is between March and May, return Spring\n",
    "    elif 3 < month <= 5:\n",
    "        return 'Spring'\n",
    "    \n",
    "    # Else If Month is between June and August, return Summer\n",
    "    elif 6 < month <= 8:\n",
    "        return 'Summer'\n",
    "    \n",
    "    # Otherwise, Return Fall\n",
    "    else:\n",
    "        return 'Fall'\n",
    "\n",
    "\n",
    "# Further aggregation is required\n",
    "# Aggregate Active Sales Properties by Census Region & Date\n",
    "groupby_usa_region = iraw_df.groupby([\"Census Region\", \"date\"])[\"value\"].sum().reset_index()\n",
    "\n",
    "# Convert date into datetime; used for plotting\n",
    "groupby_usa_region[\"date\"] = pd.to_datetime(groupby_usa_region[\"date\"])\n",
    "\n",
    "# Create new 'Season column'; pass values from 'date' to get_season function to return the Season for every row\n",
    "# Column will be used for statistical testing purposes\n",
    "groupby_usa_region['Season'] = groupby_usa_region['date'].apply(get_season)\n",
    "\n",
    "# Add 'Year' column; used for plotting\n",
    "groupby_usa_region['Year'] = groupby_usa_region['date'].dt.year\n",
    "\n",
    "# Remove rows with Year 2017 and 2018; will not be plotted\n",
    "remove_years = [2017, 2018]\n",
    "groupby_usa_region = groupby_usa_region[~groupby_usa_region[\"Year\"].isin(remove_years)]\n",
    "\n",
    "groupby_usa_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4292d166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform ANOVA test; determine if there are significant statistical differences across seasons\n",
    "def anova_test (region_df) :\n",
    "    seasons = region_df['Season'].unique()\n",
    "    \n",
    "    # For each season, store a list of For-Sale Inventory values \n",
    "    anova_dataset = [region_df[region_df['Season'] == season]['value'] for season in seasons]\n",
    "    \n",
    "    #Using SciPy, get the f-statistic and p-value for the ANOVA test \n",
    "    f_statistic, p_value = st.f_oneway(*anova_dataset)\n",
    "    \n",
    "    # Get the USA Metropolitan Region name from the DataFrame\n",
    "    region_name = ', '.join(region_df['Census Region'].unique().astype(str))\n",
    "    \n",
    "    # Print f-statistic & p values of ANOVA test\n",
    "    print(f\"{region_name} Region ANOVA F-statistic Value: {f_statistic}\")\n",
    "    print(f\"{region_name} ANOVA p-Value: {p_value}\")\n",
    "    print()\n",
    "\n",
    "    # if the p-value is significant (e.g., < 0.05) to indicate differences in seasons\n",
    "    if p_value < 0.05:\n",
    "        print(\"[REJECT Null Hypothesis (H0)]\")\n",
    "        print(f\"[{region_name} Region] With 95% confidence, there are significant statistical differences in For-Sale Inventory across seasons.\")\n",
    "        print()\n",
    "    \n",
    "    else:\n",
    "        print(\"[DO NOT REJECT NULL HYPOTHESIS (H0)]\")\n",
    "        print(f\"[{region_name} Region] At 5% Risk Level, there are NO significant statistical differences in For-Sale Inventory across seasons.\")\n",
    "        print()\n",
    "\n",
    "\n",
    "        \n",
    "# Function to change year in a date (datetime element) to 2021\n",
    "def change_year_2021 (date):\n",
    "    # For every date value...\n",
    "    try:\n",
    "        # Try to replace the year with '2021'; if successful, move on to the next and attempt\n",
    "        return date.replace(year = 2021)\n",
    "    \n",
    "    # If a 'ValueError' is triggered when attempted...\n",
    "    except ValueError:\n",
    "        \n",
    "        # Then the day in the date is out of range (e.g. Leap Year; February 29th) so subtract date by one day and attempt again\n",
    "        return (date - pd.Timedelta(days = 1)).replace(year = 2021)\n",
    "\n",
    "\n",
    "\n",
    "# Function to generate a time series plot; to be called for every USA region\n",
    "def plot_region_time_series (region_df) :\n",
    "    \n",
    "    # In new column, add 'date' element (datetime converted) where year is changed 2021 in the region DataFrame\n",
    "    # Time series is plotted using 'plot_date' as the x axis; this will help with plotting each year as their own line \n",
    "    region_df['plot_date'] = region_df['date'].apply(change_year_2021)\n",
    "    \n",
    "    # Get the USA Metropolitan Region name from the DataFrame\n",
    "    region_name = ', '.join(region_df['Census Region'].unique().astype(str))\n",
    "    \n",
    "    # Going by Meterological Seasons; USA in Northern Hemisphere\n",
    "    # Source: https://www.timeanddate.com/calendar/aboutseasons.html\n",
    "    # List of dictionaries defining a section of the time series (by x axis) to colour according to season of the year \n",
    "    season_sections = [\n",
    "        {'start_season': '2021-01-01', 'end_season': '2021-03-01', 'color': 'deepskyblue'}, # Winter Season\n",
    "        {'start_season': '2021-03-01', 'end_season': '2021-05-31', 'color': 'limegreen'},   # Spring Season\n",
    "        {'start_season': '2021-05-31', 'end_season': '2021-08-31', 'color': 'gold'},        # Summer Season\n",
    "        {'start_season': '2021-08-30', 'end_season': '2021-11-30', 'color': 'indianred'},   # Autumn Season\n",
    "        {'start_season': '2021-11-30', 'end_season': '2021-12-31', 'color': 'deepskyblue'}] # Winter Season\n",
    "\n",
    "\n",
    "    # Set Figure Size\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot the data using the 'plot_date' column\n",
    "    # For every unique 'Year' value...\n",
    "    for year in region_df['Year'].unique():\n",
    "        \n",
    "        # Extract all rows for the current Year and keep in a new DataFrame subset\n",
    "        subset = region_df[region_df['Year'] == year]\n",
    "        \n",
    "        # Using the subset, plot a line where x = 'plot_date', y = For-Sale Inventory\n",
    "        # Beautify the plot points and line itself\n",
    "        plt.plot(subset['plot_date'], subset['value'], label = year,\n",
    "                 marker = 'o', markeredgecolor = \"black\", markersize = 3.5, linewidth = 2)\n",
    "        \n",
    "        # Get the date & 'For-Sale Inventory' from the first row of the subset \n",
    "        # To be used as reference information when annotating\n",
    "        first_date = subset['plot_date'].iloc[0]\n",
    "        first_value = subset['value'].iloc[0]\n",
    "        \n",
    "        # For the current line plot, annotate the 'Year' label at the first plot point of the line.\n",
    "        # Offset it and beautify the label\n",
    "        # Add an arrow prop so that it points to the first plot point of the line\n",
    "        plt.annotate(str(year), (first_date, first_value),\n",
    "                     xytext=(-30,20), textcoords = 'offset points',\n",
    "                     fontsize = 12, color='black', weight = \"bold\",\n",
    "                     arrowprops=dict(arrowstyle = \"->\"))\n",
    "\n",
    "    \n",
    "    # Store a list of end of month dates for all of 2021\n",
    "    months = pd.date_range(start = '2021-01-01', end = '2021-12-31', freq = 'M')\n",
    "    \n",
    "    # Set the x-axis ticks and labels of the figure\n",
    "    # Range of x axis is the list of end of month dates (offset by 1 to show the lines start at Jan)\n",
    "    # Labels are the abbreviated month names\n",
    "    plt.xticks(months - pd.DateOffset(months = 1), months.strftime('%b'), rotation=45)\n",
    "    \n",
    "    # Add Title, x axis label, y axis label and beautify\n",
    "    plt.title(f'For-Sale Inventory in Metropolitan USA {region_name} Region (Raw, All Homes, Weekly 2019-2023)', fontsize  = 16)\n",
    "    plt.xlabel(\"Month\", fontsize = 14)\n",
    "    plt.ylabel(\"No. Active Properties\", fontsize = 14)\n",
    "\n",
    "    # Beautify the x/y ticks\n",
    "    plt.xticks(fontsize = 11)\n",
    "    plt.yticks(fontsize = 11)\n",
    "\n",
    "    # Add a legend and place it at the top right of the figure\n",
    "    plt.legend(title='Year', loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    \n",
    "    # Include grid lines\n",
    "    plt.grid()\n",
    "\n",
    "    # For every season dictionary...\n",
    "    for section in season_sections:\n",
    "        \n",
    "        # Get the start and end dates as well as the colour for the current season\n",
    "        start_season = section['start_season']\n",
    "        end_season = section['end_season']\n",
    "        color = section['color']\n",
    "        \n",
    "        # Set the colour background (within the axes) between the start and end dates of the 'x axis'\n",
    "        plt.axvspan(start_season, end_season, facecolor = color, alpha = 0.3)\n",
    "    \n",
    "    # Save the figure as a PNG image file\n",
    "    plt.savefig(f'Resources/Images/{region_name}.png')\n",
    "\n",
    "    # Display final time series\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28be43a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "northeast_df = groupby_usa_region[groupby_usa_region[\"Census Region\"] == \"Northeast\"]\n",
    "\n",
    "anova_test(northeast_df)\n",
    "\n",
    "plot_region_time_series(northeast_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38959286",
   "metadata": {},
   "outputs": [],
   "source": [
    "midwest_df = groupby_usa_region[groupby_usa_region[\"Census Region\"] == \"Midwest\"]\n",
    "\n",
    "anova_test(midwest_df)\n",
    "\n",
    "plot_region_time_series(midwest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cb5221",
   "metadata": {},
   "outputs": [],
   "source": [
    "south_df = groupby_usa_region[groupby_usa_region[\"Census Region\"] == \"South\"]\n",
    "\n",
    "anova_test(south_df)\n",
    "\n",
    "plot_region_time_series(south_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_df = groupby_usa_region[groupby_usa_region[\"Census Region\"] == \"West\"]\n",
    "\n",
    "anova_test(west_df)\n",
    "\n",
    "plot_region_time_series(west_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc276c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ffda188e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
